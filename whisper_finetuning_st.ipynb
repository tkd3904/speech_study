{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          audio_path  \\\n",
      "0  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "1  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "2  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "3  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "4  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "\n",
      "                                          transcript  \n",
      "0         o/ n/ ë„¤ ê°ì‚¬í•©ë‹ˆë‹¤. (NCS)/(ì—”ì”¨ì—ìŠ¤) êµìœ¡ê³¼ì • ë¬¸ì˜ ì²´í—˜ì…ë‹ˆë‹¤.  \n",
      "1                                      o/ n/ ë„¤ ì—¬ë³´ì„¸ìš”.  \n",
      "2  o/ n/ ì•„ ë„¤ ì €ê¸° ê·¸ (NCS)/(ì—”ì”¨ì—ìŠ¤) ì¸ì‚¬ë‹´ë‹¹ì ê¸°ë³¸ ì‹¬í™”ê³¼ì • ì‹ ì²­í•˜ê³ ...  \n",
      "3  o/ n/ ì•„ ë„¤ ë§ìŠµë‹ˆë‹¤. í™ˆí˜ì´ì§€ì— ë‚˜ì™€ìˆëŠ” ì–´ ë„¤, ë„¤. ê¸°ë³¸ê³¼ì •ì´ë‚˜ ì‹¬í™”ê³¼ì •...  \n",
      "4                                     o/ n/ ë„¤ ì•Œê² ìŠµë‹ˆë‹¤.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(data_dir):\n",
    "    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    data = []\n",
    "\n",
    "    # ìƒìœ„ í´ë” ìˆœíšŒ\n",
    "    for speaker_dir in os.listdir(data_dir):\n",
    "        speaker_path = os.path.join(data_dir, speaker_dir)\n",
    "        if os.path.isdir(speaker_path):\n",
    "            # í•˜ìœ„ í´ë” ë‚´ì˜ ì˜¤ë””ì˜¤ ë° í…ìŠ¤íŠ¸ íŒŒì¼ ê²€ìƒ‰\n",
    "            audio_files = glob.glob(os.path.join(speaker_path, '*.wav'))\n",
    "            for audio_file in audio_files:\n",
    "                # ì˜¤ë””ì˜¤ íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "                transcript_file = audio_file.replace('.wav', '.txt')\n",
    "                if os.path.exists(transcript_file):\n",
    "                    with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "                        transcript = f.read().strip()\n",
    "\n",
    "                    # ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "                    data.append({\n",
    "                        'audio_path': audio_file,\n",
    "                        'transcript': transcript\n",
    "                    })\n",
    "\n",
    "    # DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# ë°ì´í„° í´ë” ê²½ë¡œ\n",
    "data_dir = \"/mnt/c/Users/tkd39/stt/1.Training/D03/J13/\"\n",
    "dataset = load_data(data_dir)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('/mnt/datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/mnt/c/datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ê´€ë ¨\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Transformers ê´€ë ¨ (Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬)\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Datasets ë° í‰ê°€ ê´€ë ¨\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "# ì‹œê°í™” ë° Jupyter Notebook ê´€ë ¨\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                         audio_path  \\\n",
      "0           0  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "1           1  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "2           2  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "3           3  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "4           4  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "\n",
      "                                          transcript  \n",
      "0                          ë„¤ ê°ì‚¬í•©ë‹ˆë‹¤. / êµìœ¡ê³¼ì • ë¬¸ì˜ ì²´í—˜ì…ë‹ˆë‹¤.  \n",
      "1                                            ë„¤ ì—¬ë³´ì„¸ìš”.  \n",
      "2  ì•„ ë„¤ ì €ê¸° ê·¸ / ì¸ì‚¬ë‹´ë‹¹ì ê¸°ë³¸ ì‹¬í™”ê³¼ì • ì‹ ì²­í•˜ê³  ì‹¶ì€ë° êµìœ¡ ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜...  \n",
      "3  ì•„ ë„¤ ë§ìŠµë‹ˆë‹¤. í™ˆí˜ì´ì§€ì— ë‚˜ì™€ìˆëŠ” ì–´ ë„¤, ë„¤. ê¸°ë³¸ê³¼ì •ì´ë‚˜ ì‹¬í™”ê³¼ì • / ë‹¤ ì–´...  \n",
      "4                                           ë„¤ ì•Œê² ìŠµë‹ˆë‹¤.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def clean_transcript(text):\n",
    "    # ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°\n",
    "    text = re.sub(r'o/ n/ ', '', text)  # \"o/ n/\" ì œê±°\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)  # ê´„í˜¸ ì•ˆ í…ìŠ¤íŠ¸ ì œê±°\n",
    "    text = text.strip()  # ì•ë’¤ ê³µë°± ì œê±°\n",
    "    return text\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©\n",
    "dataset['transcript'] = dataset['transcript'].apply(clean_transcript)\n",
    "\n",
    "# 2. ë°ì´í„°ì…‹ í™•ì¸\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—´ ì´ë¦„ ë³€ê²½ (Whisperê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì— ë§ì¶”ê¸° ìœ„í•´)\n",
    "dataset = dataset.rename(columns={'audio_path': 'audio', 'transcript': 'transcription'})\n",
    "\n",
    "# ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥\n",
    "dataset.to_csv('/mnt/c/whisper_korean_cleaned_dataset.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740784c0532d49f9bb67ba0fd408dcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = load_dataset('csv', data_files='/mnt/c/whisper_korean_cleaned_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë””ì˜¤ ì—´ì„ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# ëª¨ë¸ì˜ íƒœìŠ¤í¬ì™€ ì–¸ì–´ ì„¤ì • (í•œêµ­ì–´: 'ko')\n",
    "processor.feature_extractor.language = \"ko\"\n",
    "processor.feature_extractor.task = \"transcribe\"  # ë˜ëŠ” \"translate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d446df06185e46b29ee8740badff8b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ì— ì–¸ì–´ ì½”ë“œë¥¼ ì¶”ê°€í•´ í† í¬ë‚˜ì´ì§•\n",
    "    batch[\"labels\"] = processor.tokenizer(f\"[KO] {batch['transcription']}\").input_ids\n",
    "    return batch\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš©\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c81ffa977e499aa4f4d2ab21d5a763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/97 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥\n",
    "processed_dataset.save_to_disk(\"/mnt/dwhisper_ko_processed_dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c7933ba7094b609ce82a328bc795d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# ì €ì¥ëœ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "processed_dataset = load_from_disk(\"/mnt/dwhisper_ko_processed_dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„í•  (80% í›ˆë ¨, 20% í…ŒìŠ¤íŠ¸)\n",
    "processed_dataset = processed_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# ë¶„í• ëœ ë°ì´í„°ì…‹ í™•ì¸\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    processor: PreTrainedTokenizerBase\n",
    "    padding_value: int = -100  # -100ì€ ignore_indexë¡œ ìì£¼ ì‚¬ìš©ë¨\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        # input_featuresì™€ labelsë¥¼ ë¶„ë¦¬\n",
    "        input_features = [torch.tensor(f[\"input_features\"], dtype=torch.float) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # input_featuresë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "        batch = {\n",
    "            \"input_features\": torch.stack(input_features),\n",
    "        }\n",
    "\n",
    "        # labelsë¥¼ íŒ¨ë”© ì²˜ë¦¬í•˜ì—¬ í…ì„œë¡œ ë³€í™˜\n",
    "        batch[\"labels\"] = pad_sequence(labels, batch_first=True, padding_value=self.padding_value)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„° ìƒì„±\n",
    "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ ë¡œë“œ\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # idë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id  # ignore_index ì²˜ë¦¬\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# WER, CERë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "train_losses, val_losses, wers, cers = [], [], [], []\n",
    "\n",
    "class MetricsPlotCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # í‰ê°€ ë‹¨ê³„ì—ì„œ ë¡œìŠ¤ì™€ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\n",
    "        logs = kwargs['metrics']\n",
    "        train_losses.append(logs.get(\"loss\", None))\n",
    "        val_losses.append(logs.get(\"eval_loss\", None))\n",
    "        wers.append(logs.get(\"eval_wer\", None))\n",
    "        cers.append(logs.get(\"eval_cer\", None))\n",
    "        \n",
    "        # í”Œë¡¯ ì—…ë°ì´íŠ¸\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Evaluation Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        \n",
    "        # WER and CER plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(wers, label=\"WER\")\n",
    "        plt.plot(cers, label=\"CER\")\n",
    "        plt.xlabel(\"Evaluation Step\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.legend()\n",
    "        plt.title(\"WER and CER\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/tensor/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17831/322104268.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/shin/anaconda3/envs/tensor/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/mnt/d/whisper_korean\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainerì— ì½œë°± ì¶”ê°€\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MetricsPlotCallback()]  # ì»¤ìŠ¤í…€ ì½œë°± ì¶”ê°€\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"/mnt/whisper_korean_finetuned\")\n",
    "\n",
    "# í”„ë¡œì„¸ì„œ ì €ì¥ (í† í¬ë‚˜ì´ì €ì™€ feature_extractor í¬í•¨)\n",
    "processor.save_pretrained(\"/mnt/whisper_korean_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "processor = WhisperProcessor.from_pretrained(\"/mnt/whisper_korean_finetuned\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/mnt/whisper_korean_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, dataset):\n",
    "    references = []  # ì‹¤ì œ í…ìŠ¤íŠ¸ (ground truth)\n",
    "    predictions = [] # ëª¨ë¸ ì˜ˆì¸¡ í…ìŠ¤íŠ¸\n",
    "\n",
    "    # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")  # ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "\n",
    "    for example in dataset:\n",
    "        # ì´ë¯¸ ì „ì²˜ë¦¬ëœ input_features ì‚¬ìš©\n",
    "        input_features = torch.tensor(example[\"input_features\"]).unsqueeze(0).to(\"cuda\")  # ì°¨ì› í™•ì¥ ë° GPUë¡œ ì „ì†¡\n",
    "\n",
    "        # ëª¨ë¸ ì˜ˆì¸¡ ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # labelsë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        label_ids = example[\"labels\"]\n",
    "        label_ids = [id for id in label_ids if id != -100]  # ignore_index ê°’ ì œê±°\n",
    "        reference = processor.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # ì˜ˆì¸¡ê³¼ ì‹¤ì œê°’ ì €ì¥\n",
    "        predictions.append(transcription)\n",
    "        references.append(reference)\n",
    "\n",
    "    # WER, CER ê³„ì‚°\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    cer = cer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\"WER\": wer, \"CER\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Losses: [None, None, None]\n",
      "Validation Losses: [0.4748903214931488, 0.39683955907821655, 0.38040995597839355]\n",
      "WERs: [0.39762552846470145, 0.4058493079284184, 0.40411188973185846]\n",
      "CERs: [0.26097289548635344, 0.2713437828118025, 0.2584957026362613]\n"
     ]
    }
   ],
   "source": [
    "# í˜„ì¬ ì €ì¥ëœ ë¦¬ìŠ¤íŠ¸ ê°’ë“¤ ì¶œë ¥\n",
    "print(\"Training Losses:\", train_losses)\n",
    "print(\"Validation Losses:\", val_losses)\n",
    "print(\"WERs:\", wers)\n",
    "print(\"CERs:\", cers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'WER': 0.404030810216019, 'CER': 0.25844540430427976}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, processor, test_dataset)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
