{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          audio_path  \\\n",
      "0  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "1  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "2  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "3  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "4  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "\n",
      "                                          transcript  \n",
      "0         o/ n/ 네 감사합니다. (NCS)/(엔씨에스) 교육과정 문의 체험입니다.  \n",
      "1                                      o/ n/ 네 여보세요.  \n",
      "2  o/ n/ 아 네 저기 그 (NCS)/(엔씨에스) 인사담당자 기본 심화과정 신청하고...  \n",
      "3  o/ n/ 아 네 맞습니다. 홈페이지에 나와있는 어 네, 네. 기본과정이나 심화과정...  \n",
      "4                                     o/ n/ 네 알겠습니다.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(data_dir):\n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    data = []\n",
    "\n",
    "    # 상위 폴더 순회\n",
    "    for speaker_dir in os.listdir(data_dir):\n",
    "        speaker_path = os.path.join(data_dir, speaker_dir)\n",
    "        if os.path.isdir(speaker_path):\n",
    "            # 하위 폴더 내의 오디오 및 텍스트 파일 검색\n",
    "            audio_files = glob.glob(os.path.join(speaker_path, '*.wav'))\n",
    "            for audio_file in audio_files:\n",
    "                # 오디오 파일에 대응하는 텍스트 파일 로드\n",
    "                transcript_file = audio_file.replace('.wav', '.txt')\n",
    "                if os.path.exists(transcript_file):\n",
    "                    with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "                        transcript = f.read().strip()\n",
    "\n",
    "                    # 데이터셋 리스트에 저장\n",
    "                    data.append({\n",
    "                        'audio_path': audio_file,\n",
    "                        'transcript': transcript\n",
    "                    })\n",
    "\n",
    "    # DataFrame으로 변환\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# 데이터 폴더 경로\n",
    "data_dir = \"/mnt/c/Users/tkd39/stt/1.Training/D03/J13/\"\n",
    "dataset = load_data(data_dir)\n",
    "\n",
    "# 데이터 확인\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('/mnt/datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/mnt/c/datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 관련\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Transformers 관련 (Hugging Face 라이브러리)\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Datasets 및 평가 관련\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "# 시각화 및 Jupyter Notebook 관련\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 기타 유틸리티\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                         audio_path  \\\n",
      "0           0  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "1           1  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "2           2  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "3           3  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "4           4  /mnt/c/Users/tkd39/stt/1.Training/D03/J13/S000...   \n",
      "\n",
      "                                          transcript  \n",
      "0                          네 감사합니다. / 교육과정 문의 체험입니다.  \n",
      "1                                            네 여보세요.  \n",
      "2  아 네 저기 그 / 인사담당자 기본 심화과정 신청하고 싶은데 교육 시간이 어떻게 되...  \n",
      "3  아 네 맞습니다. 홈페이지에 나와있는 어 네, 네. 기본과정이나 심화과정 / 다 어...  \n",
      "4                                           네 알겠습니다.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# 1. 텍스트 전처리 함수 정의\n",
    "def clean_transcript(text):\n",
    "    # 불필요한 접두사 제거\n",
    "    text = re.sub(r'o/ n/ ', '', text)  # \"o/ n/\" 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)  # 괄호 안 텍스트 제거\n",
    "    text = text.strip()  # 앞뒤 공백 제거\n",
    "    return text\n",
    "\n",
    "# 텍스트 전처리 적용\n",
    "dataset['transcript'] = dataset['transcript'].apply(clean_transcript)\n",
    "\n",
    "# 2. 데이터셋 확인\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 이름 변경 (Whisper가 요구하는 형식에 맞추기 위해)\n",
    "dataset = dataset.rename(columns={'audio_path': 'audio', 'transcript': 'transcription'})\n",
    "\n",
    "# 최종 데이터셋 저장\n",
    "dataset.to_csv('/mnt/c/whisper_korean_cleaned_dataset.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740784c0532d49f9bb67ba0fd408dcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset('csv', data_files='/mnt/c/whisper_korean_cleaned_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 열을 16kHz로 리샘플링\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 모델의 태스크와 언어 설정 (한국어: 'ko')\n",
    "processor.feature_extractor.language = \"ko\"\n",
    "processor.feature_extractor.task = \"transcribe\"  # 또는 \"translate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d446df06185e46b29ee8740badff8b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # 오디오 데이터 처리\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    \n",
    "    # 텍스트에 언어 코드를 추가해 토크나이징\n",
    "    batch[\"labels\"] = processor.tokenizer(f\"[KO] {batch['transcription']}\").input_ids\n",
    "    return batch\n",
    "\n",
    "# 데이터셋 전처리 적용\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c81ffa977e499aa4f4d2ab21d5a763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/97 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전처리된 데이터셋 저장\n",
    "processed_dataset.save_to_disk(\"/mnt/dwhisper_ko_processed_dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c7933ba7094b609ce82a328bc795d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# 저장된 데이터셋 로드\n",
    "processed_dataset = load_from_disk(\"/mnt/dwhisper_ko_processed_dataset2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터셋 분할 (80% 훈련, 20% 테스트)\n",
    "processed_dataset = processed_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# 분할된 데이터셋 확인\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    processor: PreTrainedTokenizerBase\n",
    "    padding_value: int = -100  # -100은 ignore_index로 자주 사용됨\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        # input_features와 labels를 분리\n",
    "        input_features = [torch.tensor(f[\"input_features\"], dtype=torch.float) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
    "\n",
    "        # input_features를 텐서로 변환\n",
    "        batch = {\n",
    "            \"input_features\": torch.stack(input_features),\n",
    "        }\n",
    "\n",
    "        # labels를 패딩 처리하여 텐서로 변환\n",
    "        batch[\"labels\"] = pad_sequence(labels, batch_first=True, padding_value=self.padding_value)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# 데이터 콜레이터 생성\n",
    "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# 평가 메트릭 로드\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "# 평가 함수 정의\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # id를 텍스트로 변환\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id  # ignore_index 처리\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# WER, CER를 저장할 리스트 초기화\n",
    "train_losses, val_losses, wers, cers = [], [], [], []\n",
    "\n",
    "class MetricsPlotCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # 평가 단계에서 로스와 메트릭 업데이트\n",
    "        logs = kwargs['metrics']\n",
    "        train_losses.append(logs.get(\"loss\", None))\n",
    "        val_losses.append(logs.get(\"eval_loss\", None))\n",
    "        wers.append(logs.get(\"eval_wer\", None))\n",
    "        cers.append(logs.get(\"eval_cer\", None))\n",
    "        \n",
    "        # 플롯 업데이트\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Evaluation Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        \n",
    "        # WER and CER plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(wers, label=\"WER\")\n",
    "        plt.plot(cers, label=\"CER\")\n",
    "        plt.xlabel(\"Evaluation Step\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.legend()\n",
    "        plt.title(\"WER and CER\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/tensor/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_17831/322104268.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/shin/anaconda3/envs/tensor/lib/python3.10/site-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/mnt/d/whisper_korean\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer에 콜백 추가\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MetricsPlotCallback()]  # 커스텀 콜백 추가\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model(\"/mnt/whisper_korean_finetuned\")\n",
    "\n",
    "# 프로세서 저장 (토크나이저와 feature_extractor 포함)\n",
    "processor.save_pretrained(\"/mnt/whisper_korean_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 저장된 모델과 프로세서 로드\n",
    "processor = WhisperProcessor.from_pretrained(\"/mnt/whisper_korean_finetuned\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/mnt/whisper_korean_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, dataset):\n",
    "    references = []  # 실제 텍스트 (ground truth)\n",
    "    predictions = [] # 모델 예측 텍스트\n",
    "\n",
    "    # 평가 모드로 전환\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")  # 모델을 GPU로 이동\n",
    "\n",
    "    for example in dataset:\n",
    "        # 이미 전처리된 input_features 사용\n",
    "        input_features = torch.tensor(example[\"input_features\"]).unsqueeze(0).to(\"cuda\")  # 차원 확장 및 GPU로 전송\n",
    "\n",
    "        # 모델 예측 생성\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # labels를 텍스트로 변환\n",
    "        label_ids = example[\"labels\"]\n",
    "        label_ids = [id for id in label_ids if id != -100]  # ignore_index 값 제거\n",
    "        reference = processor.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # 예측과 실제값 저장\n",
    "        predictions.append(transcription)\n",
    "        references.append(reference)\n",
    "\n",
    "    # WER, CER 계산\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    cer = cer_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\"WER\": wer, \"CER\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Losses: [None, None, None]\n",
      "Validation Losses: [0.4748903214931488, 0.39683955907821655, 0.38040995597839355]\n",
      "WERs: [0.39762552846470145, 0.4058493079284184, 0.40411188973185846]\n",
      "CERs: [0.26097289548635344, 0.2713437828118025, 0.2584957026362613]\n"
     ]
    }
   ],
   "source": [
    "# 현재 저장된 리스트 값들 출력\n",
    "print(\"Training Losses:\", train_losses)\n",
    "print(\"Validation Losses:\", val_losses)\n",
    "print(\"WERs:\", wers)\n",
    "print(\"CERs:\", cers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'WER': 0.404030810216019, 'CER': 0.25844540430427976}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model, processor, test_dataset)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
